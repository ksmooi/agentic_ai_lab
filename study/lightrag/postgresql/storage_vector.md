# Introducing PGVectorStorage: A PostgreSQL Vector Storage Implementation for Lightrag

**Table of Contents**

1. [Overview](#overview)  
2. [Key Features of PGVectorStorage](#key-features-of-pgvectorstorage)  
3. [Step-by-Step Code Walkthrough](#step-by-step-code-walkthrough)  
   3.1. [Initialization and Configuration](#initialization-and-configuration)  
   3.2. [Upsert Method: Inserting Vectors into PostgreSQL](#upsert-method-inserting-vectors-into-postgresql)  
   3.3. [Query Method: Retrieving Similar Vectors](#query-method-retrieving-similar-vectors)  
   3.4. [Utility Methods for Data Preparation](#utility-methods-for-data-preparation)  
4. [Summary Table of Key Methods](#summary-table-of-key-methods)  
5. [Conclusion](#conclusion)

---

## Overview

`PGVectorStorage` is a PostgreSQL-backed vector storage implementation that integrates with the Lightrag framework. It is designed to store vector embeddings (for example, those generated by an embedding function) along with associated document or entity data. The class leverages PostgreSQL for persistent storage and provides methods to upsert (insert or update) vector data and query for similar vectors based on cosine similarity.

In this article, we explain the code step by step with detailed inline comments and a summary table, so you can understand how `PGVectorStorage` works and how it fits into the overall Lightrag ecosystem.

---

## Key Features of PGVectorStorage

- **Vector Upsert:** Inserts or updates vector embeddings into a PostgreSQL database using pre-defined SQL templates.
- **Batch Embedding Generation:** Processes data in batches for efficient embedding computation.
- **Cosine Similarity Query:** Queries the vector store to find items with similarity above a specified threshold.
- **Configuration Driven:** Uses global configuration parameters for batch size and cosine threshold.

---

## Step-by-Step Code Walkthrough

### 3.1 Initialization and Configuration

The `PGVectorStorage` class is a data class that inherits from `BaseVectorStorage`. During initialization, it configures batch sizes for embedding generation and sets a threshold for cosine similarity queries.

```python
@dataclass
class PGVectorStorage(BaseVectorStorage):
    # Cosine similarity threshold loaded from environment variable
    cosine_better_than_threshold: float = float(os.getenv("COSINE_THRESHOLD", "0.2"))
    db: PostgreSQLDB = None  # PostgreSQL database instance

    def __post_init__(self):
        # Set maximum batch size for embedding generation from global config
        self._max_batch_size = self.global_config["embedding_batch_num"]
        # Load additional configuration if provided
        config = self.global_config.get("vector_db_storage_cls_kwargs", {})
        self.cosine_better_than_threshold = config.get(
            "cosine_better_than_threshold", self.cosine_better_than_threshold
        )
```

*Explanation:*  
- The `__post_init__` method is called after the data class is initialized.
- It reads the maximum batch size and cosine similarity threshold from the global configuration.
- The `db` attribute holds a reference to the PostgreSQLDB instance used for database operations.

---

### 3.2 Upsert Method: Inserting Vectors into PostgreSQL

The `upsert` method is responsible for inserting or updating vector data. It processes the input data in batches, computes embeddings for each batch, and then inserts the resulting vectors into the database.

```python
async def upsert(self, data: Dict[str, dict]):
    logger.info(f"Inserting {len(data)} vectors to {self.namespace}")
    if not len(data):
        logger.warning("You insert an empty data to vector DB")
        return []
    
    # Capture the current time to timestamp new data
    current_time = time.time()
    # Build a list of data dictionaries with extra metadata (__id__ and __created_at__)
    list_data = [
        {
            "__id__": k,
            "__created_at__": current_time,
            **{k1: v1 for k1, v1 in v.items()},
        }
        for k, v in data.items()
    ]
    
    # Extract contents to compute embeddings
    contents = [v["content"] for v in data.values()]
    # Create batches based on _max_batch_size
    batches = [
        contents[i : i + self._max_batch_size]
        for i in range(0, len(contents), self._max_batch_size)
    ]

    # Define an asynchronous task to compute embeddings for a batch
    async def wrapped_task(batch):
        result = await self.embedding_func(batch)
        pbar.update(1)
        return result

    # Create tasks for each batch and setup a progress bar using tqdm_async
    embedding_tasks = [wrapped_task(batch) for batch in batches]
    pbar = tqdm_async(
        total=len(embedding_tasks), desc="Generating embeddings", unit="batch"
    )
    # Wait for all embedding tasks to complete and concatenate the results
    embeddings_list = await asyncio.gather(*embedding_tasks)
    embeddings = np.concatenate(embeddings_list)

    # Assign each embedding to its corresponding data dictionary
    for i, d in enumerate(list_data):
        d["__vector__"] = embeddings[i]
    
    # Loop over each item and select the correct upsert method based on the namespace
    for item in list_data:
        if self.namespace == "chunks":
            upsert_sql, data = self._upsert_chunks(item)
        elif self.namespace == "entities":
            upsert_sql, data = self._upsert_entities(item)
        elif self.namespace == "relationships":
            upsert_sql, data = self._upsert_relationships(item)
        else:
            raise ValueError(f"{self.namespace} is not supported")
        
        # Execute the SQL upsert command for each item
        await self.db.execute(upsert_sql, data)
```

*Explanation:*  
- **Batch Processing:** Data is grouped into batches based on a configured maximum batch size.  
- **Embedding Generation:** For each batch, the embedding function is called asynchronously.  
- **Assignment:** The computed embeddings are then assigned back to the respective data entries.  
- **SQL Upsert:** Depending on the namespace (e.g., "chunks", "entities", "relationships"), the method selects a helper function to prepare the SQL command and executes it.

---

### 3.3 Query Method: Retrieving Similar Vectors

The `query` method is used to search the vector store for items that are similar to a given query string. It computes the embedding for the query, constructs a SQL query using that embedding, and then returns the results.

```python
async def query(self, query: str, top_k=5) -> Union[dict, list[dict]]:
    """Query the vector database for similar data."""
    # Compute the embedding for the query string
    embeddings = await self.embedding_func([query])
    embedding = embeddings[0]
    # Convert the embedding vector into a string format acceptable by SQL
    embedding_string = ",".join(map(str, embedding))

    # Construct SQL query using the embedding string and pre-defined SQL template
    sql = SQL_TEMPLATES[self.namespace].format(embedding_string=embedding_string)
    params = {
        "workspace": self.db.workspace,
        "better_than_threshold": self.cosine_better_than_threshold,
        "top_k": top_k,
    }
    # Execute the query on the database and return the results
    results = await self.db.query(sql, params=params, multirows=True)
    return results
```

*Explanation:*  
- **Query Embedding:** The provided query string is passed to the embedding function to generate a vector.  
- **SQL Construction:** The vector is formatted into a string and inserted into a SQL template to perform a similarity search based on cosine similarity.  
- **Threshold & Limit:** The parameters include a threshold value and a top_k limit to control which and how many results are returned.

---

### 3.4 Utility Methods for Data Preparation

`PGVectorStorage` includes several helper methods that prepare SQL commands and construct the corresponding data payloads for upserting various types of vector data into the PostgreSQL database. These methods ensure that the proper SQL template is selected based on the namespace (e.g., chunks, entities, relationships) and that the data is formatted correctly before executing the upsert operation.

#### Upsert Helper for Chunks

```python
def _upsert_chunks(self, item: dict):
    try:
        # Select the appropriate SQL template for inserting/updating a text chunk.
        upsert_sql = SQL_TEMPLATES["upsert_chunk"]
        # Prepare the data payload with all required fields.
        data = {
            "workspace": self.db.workspace,                     # Workspace identifier.
            "id": item["__id__"],                               # Unique identifier for the chunk.
            "tokens": item["tokens"],                           # Number of tokens in the chunk.
            "chunk_order_index": item["chunk_order_index"],     # Order index of the chunk within the full document.
            "full_doc_id": item["full_doc_id"],                 # Reference to the full document.
            "content": item["content"],                         # Text content of the chunk.
            "content_vector": json.dumps(item["__vector__"].tolist()),  # Serialized vector embedding.
        }
    except Exception as e:
        logger.error(f"Error to prepare upsert sql for chunks: {e}")
        print(item)
        raise e
    return upsert_sql, data
```

*Explanation:*  
- This method selects the SQL template for chunks (`upsert_chunk`) and constructs a payload that includes identifiers, content details, and the serialized vector embedding.  
- If an error occurs during preparation, it logs the error and re-raises the exception.

---

#### Upsert Helper for Entities

```python
def _upsert_entities(self, item: dict):
    try:
        # Select the SQL template for inserting/updating an entity.
        upsert_sql = SQL_TEMPLATES["upsert_entity"]
        # Prepare the data payload with the entity details.
        data = {
            "workspace": self.db.workspace,                     # Workspace identifier.
            "id": item["__id__"],                               # Unique identifier for the entity.
            "entity_name": item["entity_name"],                 # Name of the entity.
            "content": item["content"],                         # Description or content of the entity.
            "content_vector": json.dumps(item["__vector__"].tolist()),  # Serialized vector embedding.
        }
    except Exception as e:
        logger.error(f"Error to prepare upsert sql for entities: {e}")
        print(item)
        raise e
    return upsert_sql, data
```

*Explanation:*  
- This helper method constructs an upsert command for entity data.  
- It uses the `upsert_entity` SQL template and builds a payload that includes the workspace, entity identifier, name, content, and a JSON-serialized vector embedding.

---

#### Upsert Helper for Relationships

```python
def _upsert_relationships(self, item: dict):
    try:
        # Select the SQL template for inserting/updating a relationship.
        upsert_sql = SQL_TEMPLATES["upsert_relationship"]
        # Prepare the data payload with relationship details.
        data = {
            "workspace": self.db.workspace,                     # Workspace identifier.
            "id": item["__id__"],                               # Unique identifier for the relationship.
            "source_id": item["src_id"],                        # Identifier for the source entity.
            "target_id": item["tgt_id"],                        # Identifier for the target entity.
            "content": item["content"],                         # Description or content of the relationship.
            "content_vector": json.dumps(item["__vector__"].tolist()),  # Serialized vector embedding.
        }
    except Exception as e:
        logger.error(f"Error to prepare upsert sql for relationships: {e}")
        print(item)
        raise e
    return upsert_sql, data
```

*Explanation:*  
- This method selects the `upsert_relationship` SQL template and constructs a payload that includes identifiers for both the source and target entities, along with the relationship content and its vector embedding.  
- Like the other helpers, it logs any errors encountered and re-raises exceptions.

---

## Summary Table of Key Methods

| **Method**             | **Purpose**                                                    | **Key Features**                                                     |
|------------------------|----------------------------------------------------------------|----------------------------------------------------------------------|
| `__post_init__`        | Initialize configuration and batch settings                    | Reads global config, sets batch size, and cosine threshold           |
| `upsert`               | Insert or update vector data into PostgreSQL                    | Processes data in batches, generates embeddings, selects SQL template|
| `query`                | Query vector store for similar vectors                         | Computes query embedding, builds SQL query using cosine similarity     |
| `_upsert_chunks`       | Prepare SQL and data for upserting document chunks               | Formats the data payload for text chunk storage                      |
| `_upsert_entities`     | Prepare SQL and data for upserting entities                      | Similar helper for entity data                                         |
| `_upsert_relationships`| Prepare SQL and data for upserting relationships                  | Similar helper for relationship data                                 |

---

## Conclusion

`PGVectorStorage` is a powerful component within the Lightrag framework that bridges vector embedding computations with PostgreSQL storage. Its design allows for efficient batch processing and robust upsert operations, while its query method facilitates similarity searches based on cosine similarity. With clear configuration, helper methods, and detailed logging, `PGVectorStorage` is both maintainable and extendable.

This walkthrough has explained the class step by step, using code comments and summary tables to highlight its functionality. Whether you are storing document chunks, entity embeddings, or relationship vectors, `PGVectorStorage` provides the tools you need to manage your vector data within a PostgreSQL environment.
